{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "224a4842",
   "metadata": {},
   "source": [
    "# Model 0: Baseline Model with Standardized Preprocessing + Logistic Regression\n",
    "\n",
    "Set up a basic pipeline using **standardized preprocessing from preprocess.py** and Logistic Regression.\n",
    "\n",
    "## ðŸ”§ Steps:\n",
    "1. Import Libraries and load data using **preprocess.py functions**\n",
    "2. Preprocessing: Use **clean_text()** for standardized text cleaning\n",
    "3. Vectorization: CountVectorizer (Bag-of-Words) - maintaining original approach\n",
    "4. Model: LogisticRegression\n",
    "5. Evaluation: Accuracy, confusion matrix, classification report\n",
    "\n",
    "##  âœ… Purpose:\n",
    "Establish a working pipeline using **standardized preprocessing functions** and maintain baseline score (~70-80% accuracy expected)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf31b2c",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639417f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn libraries for modeling and evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our standardized preprocessing functions\n",
    "from preprocess import load_and_parse_data, create_train_validation_split, clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4dd074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using standardized function\n",
    "print(\"Loading training data using standardized preprocessing...\")\n",
    "train_data = load_and_parse_data('data/training_data_lowercase.csv')\n",
    "\n",
    "# Display basic information about the loaded data\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"Total articles: {len(train_data)}\")\n",
    "print(f\"Sample article structure: {list(train_data[0].keys())}\")\n",
    "\n",
    "# Show first few examples\n",
    "print(\"\\nFirst 3 articles:\")\n",
    "for i in range(3):\n",
    "    article = train_data[i]\n",
    "    text_preview = article['text'][:100] + \"...\" if len(article['text']) > 100 else article['text']\n",
    "    print(f\"  Article {i+1}: Label={article['label']}, Text='{text_preview}'\")\n",
    "\n",
    "# Check label distribution\n",
    "labels = [item['label'] for item in train_data]\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    label_name = \"Fake\" if label == 0 else \"Real\"\n",
    "    print(f\"  {label_name} (label {label}): {count} articles ({count/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0281440c",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Using Standardized Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5725d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardized text cleaning to all articles\n",
    "print(\"Applying standardized text cleaning...\")\n",
    "for item in train_data:\n",
    "    item['clean_text'] = clean_text(item['text'])\n",
    "\n",
    "# Show before/after cleaning examples\n",
    "print(\"\\nText cleaning examples:\")\n",
    "for i in range(3):\n",
    "    article = train_data[i]\n",
    "    original = article['text'][:100] + \"...\" if len(article['text']) > 100 else article['text']\n",
    "    cleaned = article['clean_text'][:100] + \"...\" if len(article['clean_text']) > 100 else article['clean_text']\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Original: '{original}'\")\n",
    "    print(f\"  Cleaned:  '{cleaned}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cd92fc",
   "metadata": {},
   "source": [
    "## 3. Train/Validation Split and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation split using standardized function\n",
    "print(\"Creating train/validation split using standardized function...\")\n",
    "\n",
    "# Prepare data for the split function\n",
    "data_for_split = []\n",
    "for item in train_data:\n",
    "    data_for_split.append({\n",
    "        'label': item['label'],\n",
    "        'text': item['clean_text']  # Use cleaned text for modeling\n",
    "    })\n",
    "\n",
    "# Use standardized train/validation split\n",
    "X_train, X_val, y_train, y_val = create_train_validation_split(\n",
    "    data_for_split, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/validation split complete:\")\n",
    "print(f\"  Training set: {len(X_train)} samples\")\n",
    "print(f\"  Validation set: {len(X_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vectorization_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization using CountVectorizer (maintaining original approach)\n",
    "\n",
    "\n",
    "print(\"Applying CountVectorizer...\")\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    lowercase=False,\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "\n",
    "print(f\"\\nVectorization complete:\")\n",
    "print(f\"  Training matrix shape: {X_train_vec.shape}\")\n",
    "print(f\"  Validation matrix shape: {X_val_vec.shape}\")\n",
    "print(f\"  Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1201e",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d53fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "import time\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "start_time = time.time()\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_vec, y_train)\n",
    "training_time_minutes = (time.time() - start_time) / 60\n",
    "\n",
    "print(\"Model training complete!\")\n",
    "print(f\"Training time: {training_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd653fd9",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940879a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on validation set\n",
    "print(\"Evaluating model performance...\")\n",
    "y_pred = model.predict(X_val_vec)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"\\n=== MODEL PERFORMANCE ===\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(\n",
    "    y_val, y_pred, \n",
    "    target_names=['Fake News (0)', 'Real News (1)'],\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355cc0f",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa805b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    model, X_val_vec, y_val, \n",
    "    cmap='Blues',\n",
    "    display_labels=['Fake News', 'Real News']\n",
    ")\n",
    "plt.title(\"Confusion Matrix - Logistic Regression with Standardized Preprocessing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_section",
   "metadata": {},
   "source": [
    "## 7. Save Results for Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation module\n",
    "from model_eval import save_model_results\n",
    "\n",
    "# Save results using clean keyword arguments\n",
    "save_model_results(\n",
    "    model_name=\"baseline_lr\",\n",
    "    display_name=\"Baseline LogisticRegression\",\n",
    "    accuracy=accuracy,\n",
    "    training_time_minutes=training_time_minutes,\n",
    "    model_architecture=\"LogisticRegression with CountVectorizer (10k features)\",\n",
    "    preprocessing_type=\"standardized_clean_text\",\n",
    "    hyperparameters={\n",
    "        \"max_features\": 10000,\n",
    "        \"stop_words\": \"english\",\n",
    "        \"min_df\": 2,\n",
    "        \"max_df\": 0.95,\n",
    "        \"max_iter\": 1000,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    dataset_info={\n",
    "        \"training_samples\": len(X_train),\n",
    "        \"validation_samples\": len(X_val),\n",
    "        \"vocabulary_size\": len(vectorizer.vocabulary_)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nModel results saved successfully for comparison!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
